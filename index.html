<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wei Bu - Portfolio</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700;800&display=swap" rel="stylesheet">

    <!-- KaTeX for LaTeX Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0KOVEMckCWLqmGNEYROJytOaOFwl0Vvr/BFnvqAaIlKmsENQTsyBI" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmImMN2hjo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>

    <style>
        body { font-family: 'Inter', sans-serif; background-color: #f8fafc; }
        .dark body { background-color: #020617; }
        #blogModal { transition: opacity 0.3s ease; }
    </style>
</head>
<body class="text-slate-800 dark:text-slate-200">

    <!-- Header / Navigation -->
    <header class="bg-white/80 dark:bg-slate-900/80 backdrop-blur-sm sticky top-0 z-50 border-b border-slate-200 dark:border-slate-800">
        <nav class="max-w-5xl mx-auto px-4 py-3 flex justify-between items-center">
            <a href="#" class="text-xl font-bold text-slate-900 dark:text-white">Wei Bu</a>
            <div class="flex items-center space-x-4 md:space-x-6 text-sm font-medium">
                <a href="#about" class="hover:text-sky-500 transition-colors">About</a>
                <a href="#publications" class="hover:text-sky-500 transition-colors">Publications</a>
                <a href="#blog" class="hover:text-sky-500 transition-colors">Blog</a>
                <button id="theme-toggle" class="p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-800 transition-colors"></button>
            </div>
        </nav>
    </header>

    <main class="max-w-5xl mx-auto px-4 py-8 md:py-16">

        <!-- ======================================================= -->
        <!-- PERSONAL INFO SECTION -->
        <!-- ======================================================= -->
        <section id="about" class="flex flex-col md:flex-row items-center gap-8 md:gap-12 mb-24">
            <div class="w-48 h-48 md:w-60 md:h-60 flex-shrink-0">
                <img src="https://placehold.co/240x240/e2e8f0/334155?text=WB" alt="Wei Bu" class="rounded-full w-full h-full object-cover border-4 border-white dark:border-slate-800 shadow-lg">
            </div>
            <div>
                <h1 class="text-4xl md:text-5xl font-extrabold mb-2 text-slate-900 dark:text-white">Hi, I'm Wei Bu</h1>
                <p class="text-lg text-slate-600 dark:text-slate-400 mb-4">
                    So far for most part of my research, I was a theoretical physicist, more recently, I have switched to more practically relevant fields such as machine learning and optimization. 
                </p>
                <div class="flex items-center gap-4 mt-6">
                    <!-- To make this work, upload your CV as "CV_Wei-Bu.pdf" to your GitHub repository -->
                    <a href="./CV_Wei-Bu.pdf" download class="bg-sky-500 hover:bg-sky-600 text-white font-bold py-2 px-6 rounded-lg transition-transform transform hover:scale-105">
                        Download CV
                    </a>
                    <div class="flex items-center gap-4">
                        <!-- Add your social media links -->
                        <a href="#" class="text-slate-500 hover:text-sky-500 dark:text-slate-400 dark:hover:text-sky-400" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" class="w-6 h-6"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg></a>
                        <a href="#" class="text-slate-500 hover:text-sky-500 dark:text-slate-400 dark:hover:text-sky-400" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" class="w-6 h-6"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></a>
                        <a href="https://scholar.google.com/citations?hl=en&user=OWqfLxgAAAAJ" target="_blank" rel="noopener noreferrer" class="text-slate-500 hover:text-sky-500 dark:text-slate-400 dark:hover:text-sky-400" aria-label="Google Scholar">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" class="w-6 h-6"><path d="M5.242 13.769L0 9.5L12 0l12 9.5l-5.242 4.269C17.548 11.249 14.978 9 12 9s-5.548 2.249-6.758 4.769zM12 10c-3.313 0-6 2.687-6 6s2.687 6 6 6s6-2.687 6-6s-2.687-6-6-6z"/></svg>
                        </a>
                    </div>
                </div>
            </div>
        </section>

        <!-- ======================================================= -->
        <!-- PUBLICATIONS SECTION -->
        <!-- ======================================================= -->
        <section id="publications" class="mb-24">
            <h2 class="text-3xl font-bold text-center mb-12 text-slate-900 dark:text-white">Featured Publications</h2>
            <div class="space-y-6">
                <!-- Publication 1 -->
                <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-6 border border-slate-200 dark:border-slate-800">
                    <h3 class="text-lg font-bold">
                        <a href="https://arxiv.org/abs/2501.09659" target="_blank" rel="noopener noreferrer" class="hover:text-sky-500 hover:underline">From Fokker-Planck to Callan-Symanzik: Evolution of weight matrices under training</a>
                    </h3>
                    <p class="text-sm text-slate-600 dark:text-slate-400 mt-1">Bu, Wei and Kol, Uri and Liu, Ziming</p>
                    <p class="text-sm text-slate-500 dark:text-slate-500 mt-2">2501.09659, 2025</p>
                </div>
                <!-- Publication 2 -->
                <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-6 border border-slate-200 dark:border-slate-800">
                    <h3 class="text-lg font-bold">
                        <a href="https://arxiv.org/abs/2412.00918" target="_blank" rel="noopener noreferrer" class="hover:text-sky-500 hover:underline">2d theory for asymptotic dynamics of 4d (self-dual) Einstein gravity</a>
                    </h3>
                    <p class="text-sm text-slate-600 dark:text-slate-400 mt-1">Bu, Wei and Seet, Sean</p>
                    <p class="text-sm text-slate-500 dark:text-slate-500 mt-2">2412.00918, 2024</p>
                </div>
                <!-- Publication 3 -->
                <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-6 border border-slate-200 dark:border-slate-800">
                    <h3 class="text-lg font-bold">
                        <a href="https://arxiv.org/abs/2310.17457" target="_blank" rel="noopener noreferrer" class="hover:text-sky-500 hover:underline">A hidden 2d CFT for self-dual Yang-Mills on the celestial sphere</a>
                    </h3>
                    <p class="text-sm text-slate-600 dark:text-slate-400 mt-1">Bu, Wei and Seet, Sean</p>
                    <p class="text-sm text-slate-500 dark:text-slate-500 mt-2">2310.17457, 2023</p>
                </div>
            </div>
        </section>
        
        <!-- ======================================================= -->
        <!-- BLOG SECTION -->
        <!-- The content is in the <script> tag at the bottom -->
        <!-- ======================================================= -->
        <section id="blog">
            <h2 class="text-3xl font-bold text-center mb-12 text-slate-900 dark:text-white">From My Blog</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                <!-- Blog Post Card 1 -->
                <div class="blog-post-card bg-white dark:bg-slate-800/50 rounded-lg shadow-md overflow-hidden cursor-pointer transform hover:-translate-y-1 transition-transform" data-id="1">
                    <img src="https://placehold.co/600x300/10b981/white?text=LaTeX" class="w-full h-48 object-cover" alt="Blog Post 1">
                    <div class="p-6">
                        <p class="text-xs text-slate-500 dark:text-slate-400 mb-1">July 30, 2025</p>
                        <h3 class="text-xl font-bold mb-2">Understanding Euler's Identity</h3>
                        <p class="text-sm text-slate-600 dark:text-slate-400">A deep dive into one of the most beautiful equations in mathematics, using LaTeX for clarity...</p>
                    </div>
                </div>
                <!-- Blog Post Card 2 -->
                <div class="blog-post-card bg-white dark:bg-slate-800/50 rounded-lg shadow-md overflow-hidden cursor-pointer transform hover:-translate-y-1 transition-transform" data-id="2">
                    <img src="https://placehold.co/600x300/f43f5e/white?text=React" class="w-full h-48 object-cover" alt="Blog Post 2">
                    <div class="p-6">
                        <p class="text-xs text-slate-500 dark:text-slate-400 mb-1">July 15, 2025</p>
                        <h3 class="text-xl font-bold mb-2">Building a Reusable React Component Library</h3>
                        <p class="text-sm text-slate-600 dark:text-slate-400">Best practices for creating a flexible, scalable, and maintainable component library in React...</p>
                    </div>
                </div>
                <!-- NEW Blog Post Card 3 - ICL -->
                <div class="blog-post-card bg-white dark:bg-slate-800/50 rounded-lg shadow-md overflow-hidden cursor-pointer transform hover:-translate-y-1 transition-transform" data-id="3">
                    <img src="https://placehold.co/600x300/8b5cf6/white?text=AI+Brain" class="w-full h-48 object-cover" alt="ICL Blog Post">
                    <div class="p-6">
                        <p class="text-xs text-slate-500 dark:text-slate-400 mb-1">August 1, 2025</p>
                        <h3 class="text-xl font-bold mb-2">How AI Models Learn Without Training: The Magic of ICL</h3>
                        <p class="text-sm text-slate-600 dark:text-slate-400">Discover how ChatGPT learns new tasks from examples - and how this fulfills Geoffrey Hinton's decades-old dream...</p>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Blog Post Modal -->
    <div id="blogModal" class="fixed inset-0 bg-black bg-opacity-70 z-50 hidden items-center justify-center p-4" onclick="closeModal()">
        <div class="bg-white dark:bg-slate-900 rounded-lg shadow-2xl w-full max-w-4xl max-h-[90vh] overflow-y-auto" onclick="event.stopPropagation()">
            <div class="sticky top-0 bg-white/80 dark:bg-slate-900/80 backdrop-blur-sm p-4 border-b border-slate-200 dark:border-slate-800 flex justify-between items-center">
                <h2 id="modal-title" class="text-2xl font-bold"></h2>
                <button onclick="closeModal()" class="p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 text-2xl leading-none">&times;</button>
            </div>
            <div id="modal-content" class="p-6 prose dark:prose-invert max-w-none"></div>
        </div>
    </div>

    <footer class="text-center py-8 mt-16 border-t border-slate-200 dark:border-slate-800">
        <p class="text-sm text-slate-500 dark:text-slate-400">&copy; 2025 Wei Bu. All rights reserved.</p>
    </footer>

    <script>
        // --- EDIT YOUR BLOG POSTS HERE ---
        const blogPosts = {
            '1': {
                title: "Understanding Euler's Identity",
                content: `
                    <p>Euler's identity is often called the most beautiful equation in mathematics. It connects five fundamental mathematical constants in a single, elegant formula:</p>
                    $$ e^{i\pi} + 1 = 0 $$
                    <p>Let's break it down. It involves:</p>
                    <ul>
                        <li>$e$, Euler's number, the base of natural logarithms.</li>
                        <li>$i$, the imaginary unit, which satisfies $i^2 = -1$.</li>
                        <li>$\pi$, the ratio of a circle's circumference to its diameter.</li>
                        <li>$1$, the multiplicative identity.</li>
                        <li>$0$, the additive identity.</li>
                    </ul>
                    <p>The proof stems from Euler's formula, which states that for any real number $x$, $e^{ix} = \cos(x) + i\sin(x)$. If we substitute $x = \pi$, we get:</p>
                    $$ e^{i\pi} = \cos(\pi) + i\sin(\pi) $$
                    <p>Since $\cos(\pi) = -1$ and $\sin(\pi) = 0$, the equation simplifies to $e^{i\pi} = -1$, which rearranges to the famous identity.</p>
                `
            },
            '3': {
                title: 'ICL: Fast Weights Through Kernel Space',
                content: `
                    <h2 class="text-2xl font-bold mt-6 mb-4">The Problem Hinton Identified</h2>
                    <p>Back in 2016, Geoffrey Hinton had this brilliant insight: neural networks should have two timescales of adaptation. Most weights should change slowly during training, but some should adapt quickly for each input.</p>
                    
                    <p>His idea was simple. Instead of just:</p>
                    $y = W x$
                    
                    <p>We should have:</p>
                    $y = (W_{\\text{slow}} + W_{\\text{fast}}(x)) x$
                    
                    <p>where $W_{\\text{slow}} \\in \\mathbb{R}^{d \\times d}$ are learned during training, and $W_{\\text{fast}}(x) \\in \\mathbb{R}^{d \\times d}$ adapt instantly based on input $x$.</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">The Computational Nightmare</h3>
                    <p>But here's the problem. With regular weights, you can batch process:</p>
                    $Y = W \\cdot X$
                    <p>where $X \\in \\mathbb{R}^{d \\times B}$ is a batch of $B$ inputs. One matrix multiplication handles everything.</p>
                    
                    <p>With fast weights, you can't batch:</p>
                    $y_i = (W_{\\text{slow}} + W_{\\text{fast}}(x_i)) x_i \\quad \\text{for each } i$
                    
                    <p>Each input needs its own weight matrix. No parallelization. Computationally dead.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Enter Linear Attention</h2>
                    <p>Let's see how linear attention works mathematically. For query $x \\in \\mathbb{R}^d$ and context $C = \\{c_1, c_2, \\ldots, c_k\\}$ where $c_i \\in \\mathbb{R}^d$:</p>

                    $h = \\frac{\\sum_{i=0}^k \\alpha_i(x) v_i}{\\sum_{i=0}^k \\alpha_i(x)}$

                    <p>where:</p>
                    $\\alpha_0(x) = x^T W_Q^T W_K x \\quad \\text{(self-attention weight)}$
                    $\\alpha_i(x) = x^T W_Q^T W_K c_i \\quad \\text{(context attention weights)}$
                    $v_0 = x W_V, \\quad v_i = c_i W_V$

                    <p>Let's check dimensions. If $x \\in \\mathbb{R}^d$, then:</p>
                    <ul>
                        <li>$W_Q, W_K \\in \\mathbb{R}^{d \\times d_k}$</li>
                        <li>$W_V \\in \\mathbb{R}^{d \\times d_v}$</li>
                        <li>$\\alpha_i(x) \\in \\mathbb{R}$ (scalars)</li>
                        <li>$v_i \\in \\mathbb{R}^{d_v}$</li>
                        <li>$h \\in \\mathbb{R}^{d_v}$</li>
                    </ul>

                    <h3 class="text-xl font-bold mt-6 mb-3">This Produces Rational Functions!</h3>
                    <p>Notice that $h$ is a rational function of $x$:</p>
                    $h = \\frac{\\text{polynomial in } x}{\\text{polynomial in } x}$

                    <p>The numerator is:</p>
                    $\\text{num} = (x^T W_Q^T W_K x)(x W_V) + \\sum_{i=1}^k (x^T W_Q^T W_K c_i)(c_i W_V)$

                    <p>This is degree-3 polynomial in $x$ (the first term $x^T M x \\cdot x = x^T M x x$ is cubic).</p>

                    <p>The denominator is:</p>
                    $\\text{den} = x^T W_Q^T W_K x + \\sum_{i=1}^k x^T W_Q^T W_K c_i$

                    <p>This is degree-2 polynomial in $x$.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Kernel Space Trick</h2>
                    <p>Here's the key insight. To make this linear, we lift $x$ to polynomial feature space:</p>
                    $\\mathbf{T}(x) = [1, x_1, x_2, \\ldots, x_1^2, x_1x_2, \\ldots, x_1^3, x_1^2x_2, \\ldots]^T$

                    <p>For $x \\in \\mathbb{R}^d$ with degree-$N$ polynomials:</p>
                    $\\mathbf{T}(x) \\in \\mathbb{R}^D \\quad \\text{where } D = \\binom{d+N}{N}$

                    <p>For degree-3 polynomials in $\\mathbb{R}^2$:</p>
                    $\\mathbf{T}(x) = [1, x_1, x_2, x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3]^T \\in \\mathbb{R}^{10}$

                    <h3 class="text-xl font-bold mt-6 mb-3">Polynomial Approximation</h3>
                    <p>Now we can approximate the rational function:</p>
                    $h \\approx A(C) \\cdot \\mathbf{T}(x)$

                    <p>where $A(C) \\in \\mathbb{R}^{d_v \\times D}$ is the context-dependent polynomial coefficient matrix.</p>

                    <p>Let's check dimensions:</p>
                    <ul>
                        <li>$A(C) \\in \\mathbb{R}^{d_v \\times D}$</li>
                        <li>$\\mathbf{T}(x) \\in \\mathbb{R}^D$</li>
                        <li>$A(C) \\cdot \\mathbf{T}(x) \\in \\mathbb{R}^{d_v}$ ‚úì</li>
                    </ul>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Fast Weight Realization</h2>
                    <p>Now consider the MLP that follows attention:</p>
                    $f(x, C) = W_2 \\sigma(W_1 h + b_1) + b_2$

                    <p>Substituting our approximation:</p>
                    $f(x, C) = W_2 \\sigma(W_1 A(C) \\mathbf{T}(x) + b_1) + b_2$

                    <p>Define the <strong>context-modified weight</strong>:</p>
                    $\\tilde{W}_1(C) = W_1 A(C)$

                    <p>Then:</p>
                    $f(x, C) = W_2 \\sigma(\\tilde{W}_1(C) \\mathbf{T}(x) + b_1) + b_2$

                    <h3 class="text-xl font-bold mt-6 mb-3">Dimensional Analysis</h3>
                    <p>Let's verify all dimensions work:</p>
                    <ul>
                        <li>$W_1 \\in \\mathbb{R}^{d_{\\text{hidden}} \\times d_v}$ (original MLP weight)</li>
                        <li>$A(C) \\in \\mathbb{R}^{d_v \\times D}$ (polynomial coefficients)</li>
                        <li>$\\tilde{W}_1(C) = W_1 A(C) \\in \\mathbb{R}^{d_{\\text{hidden}} \\times D}$ (modified weight)</li>
                        <li>$\\mathbf{T}(x) \\in \\mathbb{R}^D$ (polynomial features)</li>
                        <li>$\\tilde{W}_1(C) \\mathbf{T}(x) \\in \\mathbb{R}^{d_{\\text{hidden}}}$ ‚úì</li>
                    </ul>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Hinton's Dream Realized</h2>
                    <p>Look what we've achieved! We have:</p>
                    $\\tilde{W}_1(C) = W_1 A(C)$

                    <p>This is exactly Hinton's vision:</p>
                    <ul>
                        <li><strong>Slow weights</strong>: $W_1$ (learned during pretraining)</li>
                        <li><strong>Fast weights</strong>: $A(C)$ (context-dependent modification)</li>
                        <li><strong>Total effect</strong>: $\\tilde{W}_1(C)$ combines both</li>
                    </ul>

                    <p>But instead of $W_{\\text{slow}} + W_{\\text{fast}}(x)$, we have $W_{\\text{slow}} \\cdot A_{\\text{fast}}(C)$.</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">Why This Solves the Computational Problem</h3>
                    <p>The genius is that attention computes $A(C)$ efficiently:</p>

                    <ol class="list-decimal list-inside space-y-2">
                        <li><strong>Parallel attention computation</strong>: $\\text{softmax}(QK^T/\\sqrt{d})$ for all positions simultaneously</li>
                        <li><strong>Batch-friendly</strong>: Same attention weights applied across batch dimension</li>
                        <li><strong>Hardware optimized</strong>: Standard matrix operations</li>
                    </ol>

                    <p>Instead of:</p>
                    <pre class="bg-gray-100 dark:bg-gray-800 p-3 rounded text-sm">
# Hinton's original (slow):
for i, x_i in enumerate(batch):
    W_fast_i = compute_fast_weights(x_i)
    y_i = (W_slow + W_fast_i) @ x_i
                    </pre>

                    <p>Transformers do:</p>
                    <pre class="bg-gray-100 dark:bg-gray-800 p-3 rounded text-sm">
# ICL (fast):
A = attention(Q, K, V)  # Batched
H = A @ V              # Batched  
Y = W_slow @ H         # Batched
                    </pre>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Concrete Example</h2>
                    <p>Let's work through a 2D example. Suppose $x = [x_1, x_2]^T$ and we use degree-2 features:</p>
                    $\\mathbf{T}(x) = [1, x_1, x_2, x_1^2, x_1x_2, x_2^2]^T$

                    <p>Context $C = \\{c_1\\}$ where $c_1 = [1, 0]^T$. The attention computation gives:</p>
                    $\\alpha_0(x) = x^T W_Q^T W_K x = x_1^2 + x_2^2 \\quad \\text{(assuming } W_Q^T W_K = I\\text{)}$
                    $\\alpha_1(x) = x^T W_Q^T W_K c_1 = x_1$

                    <p>So:</p>
                    $h = \\frac{(x_1^2 + x_2^2) \\cdot x W_V + x_1 \\cdot c_1 W_V}{x_1^2 + x_2^2 + x_1}$

                    <p>This rational function gets approximated as:</p>
                    $h \\approx [a_0(C), a_1(C), a_2(C), a_3(C), a_4(C), a_5(C)] \\cdot [1, x_1, x_2, x_1^2, x_1x_2, x_2^2]^T$

                    <p>The coefficients $a_i(C)$ depend on the context through the specific form of the rational function.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Beyond Hinton's Vision</h2>
                    <p>ICL actually exceeds Hinton's original dream in several ways:</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">1. Exponentially More Parameters</h3>
                    <p>Original space: $W_1 \\in \\mathbb{R}^{d_{\\text{hidden}} \\times d_v}$ has $d_{\\text{hidden}} \\cdot d_v$ parameters.</p>
                    <p>Lifted space: $\\tilde{W}_1(C) \\in \\mathbb{R}^{d_{\\text{hidden}} \\times D}$ has $d_{\\text{hidden}} \\cdot D$ parameters.</p>
                    <p>Since $D = \\binom{d+N}{N}$ grows exponentially with degree $N$, ICL has access to exponentially more adaptation parameters!</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">2. Nonlinear Feature Interactions</h3>
                    <p>Hinton's vision: $W_{\\text{slow}} + W_{\\text{fast}}(x)$ (additive)</p>
                    <p>ICL reality: $W_{\\text{slow}} \\cdot A_{\\text{fast}}(C)$ operating on polynomial features (multiplicative + nonlinear)</p>

                    <p>ICL can capture interactions like $x_i x_j$ that are impossible with simple weight addition.</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">3. Context-Dependent Architecture</h3>
                    <p>ICL doesn't just change weights‚Äîit changes the effective neural architecture through the polynomial feature selection encoded in $A(C)$.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Softmax Extension</h2>
                    <p>For softmax attention, we have:</p>
                    $A_i(x, C) = \\frac{\\exp(x^T W_Q^T W_K c_i / \\tau)}{\\sum_j \\exp(x^T W_Q^T W_K c_j / \\tau)}$

                    <p>Using Taylor expansion of exponentials:</p>
                    $\\exp(z) \\approx 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots$

                    <p>This gives us rational functions with higher-degree polynomials, requiring more polynomial features. The temperature $\\tau$ controls the approximation quality:</p>
                    <ul>
                        <li><strong>High $\\tau$</strong>: Nearly linear, few features needed</li>
                        <li><strong>Low $\\tau$</strong>: Highly nonlinear, many features needed</li>
                    </ul>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Bottom Line</h2>
                    <p>Transformers didn't just fulfill Hinton's dream of fast and slow weights‚Äîthey revolutionized it. Through the mathematical structure of attention operating in polynomial kernel space, they achieved:</p>

                    <ol class="list-decimal list-inside space-y-2">
                        <li><strong>Computational efficiency</strong>: Batchable attention instead of per-sample weight computation</li>
                        <li><strong>Exponential expressivity</strong>: Access to polynomial feature interactions</li>
                        <li><strong>Dynamic architecture</strong>: Context-dependent neural pathway selection</li>
                        <li><strong>Hardware optimization</strong>: Matrix operations optimized in modern accelerators</li>
                    </ol>

                    <p>The equation $\\tilde{W}_1(C) = W_1 A(C)$ with $A(C)$ computed via attention is the elegant solution to Hinton's decades-old challenge. It's mathematics at its most beautiful‚Äîsolving an impossible computational problem through the right abstraction.</p>

                    <hr class="my-8">
                    <p class="text-sm text-gray-600 dark:text-gray-400"><em>This analysis reveals why in-context learning works so well: it's not just pattern matching, but genuine fast weight adaptation in exponentially high-dimensional polynomial feature space.</em></p>
                `
            },
            '3': {
                title: 'How AI Models Learn Without Training: The Magic of ICL',
                content: `
                    <p class="text-lg font-medium text-slate-700 dark:text-slate-300 mb-6">Have you ever wondered how ChatGPT can suddenly become a French translator, a code debugger, or a creative writer just by seeing a few examples? The answer lies in a fascinating phenomenon called <strong>In-Context Learning (ICL)</strong> - and it's the fulfillment of a decades-old dream by AI pioneer Geoffrey Hinton.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Magic Trick</h2>
                    <p>Imagine you're a master chef, but instead of learning new recipes through years of practice, you could instantly adapt your cooking style just by watching someone make a dish once. That's essentially what large language models do with In-Context Learning.</p>
                    
                    <div class="bg-blue-50 dark:bg-slate-800 p-4 rounded-lg my-6">
                        <p><strong>Example:</strong> You show ChatGPT:</p>
                        <p class="font-mono text-sm mt-2">
                        üçé Apple ‚Üí Red<br>
                        üçå Banana ‚Üí Yellow<br>
                        üçä Orange ‚Üí ?
                        </p>
                        <p class="mt-2">And it immediately understands: "Ah, you want me to map fruits to their colors!" and responds with "Orange."</p>
                    </div>

                    <p>No retraining. No gradient updates. No backpropagation. The model just <em>gets it</em> from the context.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Hinton's Decades-Old Dream</h2>
                    <p>Back around 2016, Geoffrey Hinton (the "Godfather of AI") had a revolutionary idea. He believed neural networks needed <strong>two timescales</strong>:</p>
                    
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 my-6">
                        <div class="bg-green-50 dark:bg-green-900/20 p-4 rounded-lg">
                            <h4 class="font-bold text-green-800 dark:text-green-300">üêå Slow Weights</h4>
                            <p class="text-sm mt-2">Learned during training over millions of examples. Like your core knowledge and skills.</p>
                        </div>
                        <div class="bg-orange-50 dark:bg-orange-900/20 p-4 rounded-lg">
                            <h4 class="font-bold text-orange-800 dark:text-orange-300">‚ö° Fast Weights</h4>
                            <p class="text-sm mt-2">Adapted instantly for each new input. Like your ability to quickly adjust to new situations.</p>
                        </div>
                    </div>

                    <p>The idea was brilliant: $W_{total} = W_{slow} + W_{fast}(x)$, where different inputs would get different "fast weights" added to the base "slow weights."</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">The Computational Nightmare</h3>
                    <p>But there was a massive problem. Here's what Hinton's vision would require:</p>
                    
                    <div class="bg-red-50 dark:bg-red-900/20 p-4 rounded-lg my-4 font-mono text-sm">
                        # Hinton's dream (computationally expensive):<br>
                        for each input x_i in batch:<br>
                        &nbsp;&nbsp;W_fast_i = compute_fast_weights(x_i)<br>
                        &nbsp;&nbsp;W_total_i = W_slow + W_fast_i<br>
                        &nbsp;&nbsp;y_i = W_total_i @ x_i  # Can't batch this!
                    </div>

                    <p>Every input needs different weights, so you can't use efficient batch processing. It would be <strong>impossibly slow</strong>.</p>

                    <h2 class="text-2xl font-bold mt-8 mb-4">How Transformers Solved the Impossible</h2>
                    <p>Here's where the story gets beautiful. Transformers found an elegant way to achieve Hinton's dream without the computational nightmare.</p>

                    <h3 class="text-xl font-bold mt-6 mb-3">The Attention Breakthrough</h3>
                    <p>Instead of changing the weights directly, transformers change <strong>what the weights act on</strong>. It's like instead of getting a new hammer for each nail, you reshape the nail so your existing hammer works perfectly.</p>

                    <div class="bg-blue-50 dark:bg-slate-800 p-4 rounded-lg my-6">
                        <h4 class="font-bold mb-2">The Mathematical Magic:</h4>
                        <p>Instead of: $y = (W_{slow} + W_{fast}(x)) \cdot x$</p>
                        <p>Transformers do: $y = W_{slow} \cdot \text{attention}(x, \text{context})$</p>
                        <p class="text-sm mt-2 text-slate-600 dark:text-slate-400">Same effect, but the attention mechanism is efficiently batchable!</p>
                    </div>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Secret: Polynomial Feature Magic</h2>
                    <p>Here's where it gets really deep. Our mathematical analysis reveals that ICL is secretly operating in a <strong>lifted polynomial feature space</strong>.</p>

                    <p>When you give a model examples like:</p>
                    <ul class="my-4">
                        <li>"cat" ‚Üí "animal" </li>
                        <li>"dog" ‚Üí "animal"</li>
                        <li>"rose" ‚Üí ?</li>
                    </ul>

                    <p>The model isn't just pattern matching. It's:</p>
                    <ol class="list-decimal list-inside my-4 space-y-2">
                        <li><strong>Lifting your input</strong> to a high-dimensional polynomial feature space: $x \mapsto [1, x_1, x_2, x_1^2, x_1x_2, x_2^2, ...]$</li>
                        <li><strong>Using context to determine</strong> which polynomial combinations matter: $h = A(C) \cdot \text{features}(x)$</li>
                        <li><strong>Effectively modifying the weights</strong> to: $\tilde{W}(C) = W \cdot A(C)$</li>
                    </ol>

                    <div class="bg-purple-50 dark:bg-purple-900/20 p-4 rounded-lg my-6">
                        <p class="font-bold text-purple-800 dark:text-purple-300">üéØ Key Insight:</p>
                        <p class="text-sm mt-2">ICL has access to polynomial interactions (like $x_i \cdot x_j$) that regular gradient descent can't reach. That's why it can learn patterns that would be impossible with traditional training!</p>
                    </div>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Beyond Hinton's Wildest Dreams</h2>
                    <p>Not only did transformers fulfill Hinton's vision - they exceeded it:</p>

                    <div class="overflow-x-auto my-6">
                        <table class="min-w-full text-sm">
                            <tr class="border-b">
                                <th class="text-left p-2 font-bold">Hinton's Vision</th>
                                <th class="text-left p-2 font-bold">ICL Reality</th>
                            </tr>
                            <tr class="border-b">
                                <td class="p-2">Fast weights per input</td>
                                <td class="p-2">Fast <em>architecture</em> per context ‚ú®</td>
                            </tr>
                            <tr class="border-b">
                                <td class="p-2">Linear adaptation</td>
                                <td class="p-2">Nonlinear through polynomial lifting üöÄ</td>
                            </tr>
                            <tr class="border-b">
                                <td class="p-2">Computational bottleneck</td>
                                <td class="p-2">Efficiently batchable üí®</td>
                            </tr>
                            <tr>
                                <td class="p-2">Fixed feature space</td>
                                <td class="p-2">Dynamic feature expansion üéØ</td>
                            </tr>
                        </table>
                    </div>

                    <h2 class="text-2xl font-bold mt-8 mb-4">Why This Matters</h2>
                    <p>Understanding ICL reveals something profound about intelligence itself. It's not just about memorizing patterns - it's about <strong>dynamic architecture reconfiguration</strong>.</p>

                    <p>When you show ChatGPT a few examples, you're not just giving it data. You're:</p>
                    <ul class="my-4 space-y-1">
                        <li>üîß <strong>Reconfiguring its computational pathways</strong></li>
                        <li>üé® <strong>Activating specific polynomial feature combinations</strong></li>
                        <li>üéØ <strong>Creating a task-specific neural architecture on the fly</strong></li>
                    </ul>

                    <h2 class="text-2xl font-bold mt-8 mb-4">The Future</h2>
                    <p>We've only scratched the surface. Future AI systems might:</p>
                    <ul class="my-4 space-y-2">
                        <li><strong>Explicit fast weights:</strong> Direct implementation of Hinton's vision with better hardware</li>
                        <li><strong>Hybrid approaches:</strong> Combine attention with explicit weight modulation</li>
                        <li><strong>Meta-architectures:</strong> Learn to modify entire network structures, not just weights</li>
                    </ul>

                    <div class="bg-gradient-to-r from-blue-50 to-purple-50 dark:from-slate-800 dark:to-slate-700 p-6 rounded-lg my-8">
                        <p class="font-bold text-lg">üéâ The Bottom Line</p>
                        <p class="mt-2">Hinton's dream of fast and slow weights wasn't just fulfilled - it was <strong>revolutionized</strong>. Transformers found a way to achieve rapid adaptation that's more powerful, more efficient, and more elegant than anyone imagined.</p>
                        <p class="mt-2 text-sm italic">And that's why ChatGPT can learn to be a poet, programmer, or philosopher just from a few examples. It's not magic - it's mathematics. Beautiful, deep mathematics.</p>
                    </div>

                    <hr class="my-8">
                    <p class="text-sm text-slate-600 dark:text-slate-400"><em>Want to dive deeper into the mathematics? Check out the rigorous derivations in our technical papers on polynomial feature lifting and kernel space analysis of attention mechanisms.</em></p>
                `
            }
        };

        // --- Modal & LaTeX Rendering Logic ---
        const modal = document.getElementById('blogModal');
        const modalTitle = document.getElementById('modal-title');
        const modalContent = document.getElementById('modal-content');

        document.querySelectorAll('.blog-post-card').forEach(card => {
            card.addEventListener('click', () => {
                const postId = card.getAttribute('data-id');
                const post = blogPosts[postId];
                if (post) {
                    modalTitle.textContent = post.title;
                    modalContent.innerHTML = post.content;
                    // Render LaTeX after content is set
                    renderMathInElement(modalContent, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                        ]
                    });
                    modal.classList.remove('hidden');
                    modal.classList.add('flex');
                    document.body.style.overflow = 'hidden';
                }
            });
        });

        function closeModal() {
            modal.classList.add('hidden');
            modal.classList.remove('flex');
            document.body.style.overflow = '';
        }

        // --- Dark Mode Toggle ---
        const themeToggle = document.getElementById('theme-toggle');
        const sunIcon = `<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>`;
        const moonIcon = `<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>`;

        const applyTheme = (theme) => {
            if (theme === 'dark') {
                document.documentElement.classList.add('dark');
                themeToggle.innerHTML = sunIcon;
            } else {
                document.documentElement.classList.remove('dark');
                themeToggle.innerHTML = moonIcon;
            }
        };
        themeToggle.addEventListener('click', () => {
            const isDark = document.documentElement.classList.contains('dark');
            localStorage.setItem('theme', isDark ? 'light' : 'dark');
            applyTheme(isDark ? 'light' : 'dark');
        });
        const savedTheme = localStorage.getItem('theme') || (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light');
        applyTheme(savedTheme);
    </script>
</body>
</html>
